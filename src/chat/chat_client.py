import src.common.GlobalSetting as gl
from ollama import Client
from langchain_community.chat_models import ChatOllama
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.agents import initialize_agent, AgentType
import json
from src.tools.tavily_client import tavily_client
from dotenv import load_dotenv
load_dotenv()

llm_client = Client(host="http://127.0.0.1:11434")
model_name = gl.MODEL_NAME


llm = ChatOllama(
    model="llama3", 
    base_url="http://127.0.0.1:11434"
)

search_tool = TavilySearchResults(max_results=2)

# Function-style Agent (ë¹ ë¥¸ íˆ´ ì²˜ë¦¬)
agent = initialize_agent(
    tools=[search_tool],
    llm=llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    handle_parsing_errors=True
)



def run_tavily_and_get_urls(query: str, max_results: int = 3):
    try:
        res = tavily_client.search(query, max_results=max_results)
        urls = [item["url"] for item in res.get("results", []) if "url" in item]
        return urls
    except Exception as e:
        print("[Tavily Error]", e)
        return []

def stream_chat(mem, messages):
    """
    - íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì‹œ Tavily ì‹¤í–‰ í›„ URL í¬í•¨ ë‹µë³€
    - ê·¸ ì™¸ì—ëŠ” Ollama ìŠ¤íŠ¸ë¦¬ë°
    """
    user_text = messages[-1]["content"]

    # --- í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ë¶„ê¸° ---
    if any(keyword in user_text for keyword in ["ê²€ìƒ‰", "ë§›ì§‘", "ìœ„ì¹˜", "ì–´ë””", "ìƒµ", "shop", "store", "restaurant", "news"]):
        urls = run_tavily_and_get_urls(user_text, max_results=3)

        if urls:
            # Tavily URLë§Œ í¬í•¨ëœ ë‹µë³€
            answer = "ğŸ”— ê´€ë ¨ ì •ë³´ë¥¼ ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”:\n" + "\n".join(urls)
        else:
            answer = "ê´€ë ¨ëœ URLì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ğŸ˜¢"

        mem.add_assistant(answer)
        yield answer
        return

    # --- ì¼ë°˜ Ollama ìŠ¤íŠ¸ë¦¬ë° ---
    chunks = []
    for chunk in llm_client.chat(
        model=model_name,
        messages=messages,
        stream=True,
        options={"num_predict": 128}
    ):
        if isinstance(chunk, tuple):
            chunk = chunk[0]
        if "message" in chunk and "content" in chunk["message"]:
            piece = chunk["message"]["content"]
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            piece = piece.replace("<|system|>", "").replace("<|user|>", "").replace("<|assistant|>", "")
            chunks.append(piece)
            yield piece

    assistant_text = "".join(chunks).strip()
    mem.add_assistant(assistant_text)



# def stream_chat(mem, messages):
#     """
#     - íˆ´ í•„ìš” ì—†ëŠ” ê²½ìš°: Ollamaë¡œ ë°”ë¡œ ìŠ¤íŠ¸ë¦¬ë°
#     - íˆ´ í•„ìš”í•  ìˆ˜ ìˆëŠ” ê²½ìš°: run_with_tools í˜¸ì¶œ
#     """
#     user_text = messages[-1]["content"]

#     # ê°„ë‹¨í•œ ë¶„ê¸° (íˆ´ íŠ¸ë¦¬ê±° í‚¤ì›Œë“œ ì˜ˆì‹œ)
#     if any(keyword in user_text for keyword in ["ê²€ìƒ‰", "ë§›ì§‘", "ë‰´ìŠ¤", "search", "restaurant", "news"]):
#         answer = run_with_tools(user_text)
#         mem.add_assistant(answer)
#         yield answer
#         return

#     # ì¼ë°˜ ëŒ€í™” â†’ Ollama ìŠ¤íŠ¸ë¦¬ë°
#     chunks = []
#     for chunk in llm_client.chat(
#         model=model_name,
#         messages=messages,
#         stream=True,
#         options={"num_predict": 128}
#     ):
#         if isinstance(chunk, tuple):
#             chunk = chunk[0]
#         if "message" in chunk and "content" in chunk["message"]:
#             piece = chunk["message"]["content"]
#             piece = piece.replace("<|system|>", "").replace("<|user|>", "").replace("<|assistant|>", "")
#             chunks.append(piece)
#             yield piece

#     assistant_text = "".join(chunks).strip()
#     mem.add_assistant(assistant_text)

# def stream_chat(mem, messages):
#     chunks = []
#     for chunk in llm_client.chat(
#         model=model_name,
#         messages=messages,
#         stream=True,
#         options={"num_predict": 128}
#     ):
#         if isinstance(chunk, tuple): chunk = chunk[0]
#         if "message" in chunk and "content" in chunk["message"]:
#             piece = chunk["message"]["content"]

#             # ì˜¬ë¼ë§ˆëŠ” ê¸°ë³¸ì ìœ¼ë¡œ <|system|> ë¥¼ ë¶™ì´ë ¤ëŠ” ì„±ê²©ì´ ìˆìŒ ê°•ì œ ì œê±° 
#             piece = piece.replace("<|system|>", "").replace("<|user|>", "").replace("<|assistant|>", "")
#             chunks.append(piece)
#             yield piece


#     # ìºì‹±: ìŠ¤íŠ¸ë¦¬ë° ëë‚˜ê³ ë§Œ Redisì— ìµœì¢… ë‹µë³€ ì €ì¥
#     assistant_text = "".join(chunks).strip()
#     assistant_text = assistant_text.replace("<|system|>", "").replace("<|user|>", "").replace("<|assistant|>", "")
#     mem.add_assistant(assistant_text)
